* # LESSON A-1

Overview of Assessing
Research Literature

As mentioned in the home page of this Web site, the research literature can provide a useful mountain lookout that broadens and deepens your perspective of a topic and allows you to scout promising routes to distant objectives, but it can also be a snake pit filled with groundless rhetoric, invalid findings, and misleading conclusions.  In addition, the available research on a given topic usually varies some in the questions or hypotheses addressed, the contexts in which the research is conducted, the interventions that are applied in experiments and demonstrations, and the research methods used.  All these can affect the results and thus should be taken into account during the next step of the review process, synthesis across studies.
What causes invalid results and misleading conclusions in research studies?  Some are caused by incompetence, when the researchers are not adequately prepared to conduct the study.  Some are due to constraints in resources (money, staff, time) and access that force scholars to conduct research that is less thorough and rigorous than they would desire.  Some result from honest mistakes and others from carelessness. Finally some are due to duplicity--deliberately skewing the research so that it serves only to buttress preconceived opinions. 

Most research journal editors have two knowledgeable scholars review each article submitted for publication.  This is supposed to identify weaknesses and used either to request improvements or to reject the submitted article.  Ulrich's International Guide to periodicals indicates which use peer review.  University presses and commercial publishers of text books usually require peer review; most other commercial publishers do not use it.

Peer review definitely helps screen out the worst research, but there is compelling evidence that it allows considerable flawed research to reach print.  For instance, in a study of 114 research articles sampled from 44 journals, the average expert ratings of the problem statements, research procedures, data analysis, and summary and conclusion were each “mediocre” (Ward, Hall, and Schramm, 1975).

The merit of a research study can be considered like a chain hanging from a stout crossbeam, holding up a heavy pallet of knowledge.  If any one link has a major crack, the pallet is in danger of crashing to the floor.  The links include the background preparation for the study, the conceptual framework, the questions or hypotheses addressed, the contexts prevailing during the study, the interventions actually applied in experiments and demonstrations, the methodology of the study, the results, and the conclusions inferred from the study. 

Appraisal Guides

The following are some general guidelines that should help when assessing individual research studies.  The guidelines will be discussed broadly so that they are generally applicable to both quantitative and qualitative research, although not every discussion will be applicable to all research approaches.

Titles sometimes have more hype than substance.  Don’t assume the title of a research article accurately reflects what was studied or the results.

 
Try to assess the most thorough report of the research study.  Often there are three levels of reports: one intended mostly for policy makers and practitioners and focusing mostly on the interventions, results, and conclusions; one published in a research journal with moderate detail about the background, questions or hypotheses, methodology, and the results and conclusions; and one submitted to a funding agency with the most extensive detailing of the study.

 
Not all cracks in the chain links will cause the pallet to fall.  Whether the chain will hold the pallet depends on the strength of the links, the depth of the cracks, and the weight of the pallet.  Just as it is a mistake to assume every published research study can be relied upon, it is a mistake to assume that any shortcoming in a study makes it useless.  Careful and wise judgment is needed when assessing the merit of individual research studies.

 
Beware of your own biases.  Even seasoned scholars are quicker to identify the flaws in studies with results that contradict their opinions than those with results that support their opinions.  Work consciously to exercise even-handed judgment.

 
There are at least three levels of assessment.  These include: 
What are the characteristics of the study that may have affected the results?  What are the implications?  These questions are important to the synthesis phase of the review. 
Does the study validly answer the questions addressed or validly test the hypotheses posed?  This is often referred to as “internal validity.” 
Are the results sufficiently large and/or generalizable to be important to theory, policy, and/or practice?  This is often referred to as external validity.

 
At the end of each Lesson A-2 through A-8 there are one or more “Key Appraisal Questions. ”  These will help you make a good assessment of the merit of each individual study that has been found during a literature search.  The answers to these questions will also be important for the third phase of a literature review: the synthesis phase.  Depending on the priority purposes of your review and the nature of the literature being reviewed, it may be important to address additional questions or sub-questions during the appraisal of each study.

 
When assessing multiple research studies, code answers to the appraisal questions in a matrix.  The rows in the matrix should represent the studies and the columns the appraisal questions, or vice versa. 
Other Assessment Lessons
This is the first of eight brief lessons on assessing research literature. The other lessons are indicated below. For moderate competency in conducting literature reviews, one should master all the lessons. 

Lesson A-2: Assessing the background of the study
Lesson A-3: Assessing the questions or hypotheses addressed
Lesson A-4: Assessing the contexts of the situation studied
Lesson A-5: Assessing the interventions (when applicable) that were studied
Lesson A-6: Assessing the methodology of the study
Lesson A-7: Assessing the results of the study
Lesson A-8: Assessing the conclusions of the study 
Treasure Chest on Assessment 

Reference
Ward, A. W., Hall, B. W., & Schramm, C. F. (1975). Evaluation of Published Educational Research: A National Survey. American Educational Research Journal, 12 (2), 109-128.

* # LESSON A-2

Assessing the Background of the Study

Scholarly research usually draws on prior studies, a conceptual framework or theory, and/or identified problems.  These provide a rationale and focus for the study.  When the background work is done well, the focus can put the study on the cutting edge of knowledge; when done poorly the study may repeat well-settled questions or pursue an inquiry that has proved fruitless.  While focus allows for intensification of the investigation, it also inevitably delimits the generalizations that can be made from the study. 
Most research is based partly on a review of prior research.  The review may be used to test a speculation before devoting great time and expense to researching it, or the review may have provoked the research by identifying a problem that could benefit from research.  The review can also be used to scout conceptual frameworks, theories, and methodologies that might be used in the research.  If the research article or detailed report does not reference prior research on the topic, there is reason to worry that the investigator was an amateur or did a “quickie” study. 

The terms “theory” and “conceptual framework” are used in various way by researchers.  Usually a conceptual framework refers to a set concepts applicable to a given phenomena that provide an special perspective of it.  Theory usually refers to a conceptual framework that specifies causal relationships and which has been inferred from prior research and then verified with further research.  New areas of research and much survey research directed at characterizing socio-demographic characteristics and opinions about current events will have no associated conceptual framework or theory.

The most productive research is usually in response to a problem rather than mere curiosity.  The problem makes the research of importance.  The problem may be a gap in established knowledge, an apparent inconsistency or contradiction in present knowledge, the lack of a conceptual framework, or inadequacies in a theory.  In qualitative research, the problem may be foreshadowed-that is suggested but not clearly evident.  The problem is not always stated explicitly or clearly in research reports, but sometimes it is implicit.

The background work for research provides the focus and lens for the research.  This allows the investigator to see more acutely.  But it also limits peripheral vision and thus bounds or “delimits” the study.  Those delimitations are important for interpreting the results.  It is not uncommon for the conclusions and recommendations of research reports to exceed the delimitations of the study.

Assessment Questions Key Assessment Questions
What is the problem that the study is addressing?
What conceptual frameworks or theories informed the study? 

* # LESSON A-3

Assessing the Questions or 
Hypotheses Addressed

Questions or hypotheses provide further focus to a study.  In qualitative research, the investigation may begin with a few or no questions, and generate questions from the observations.  Survey research often has a few general questions, each of which are investigated with several additional more specific questions.  Experimental research may start with questions, but translates them into hypotheses, which state tentative answers to specific questions of interest, usually postulating no difference, and then test the hypotheses. 

There are four common types of research questions, and the type of question has critical implications for the appropriate research design. 

Descriptive questions (about the characteristics of something).  Example: What is the average math achievement of the nation’s fourth graders?  What is the career satisfaction of DC computer industry professional employees?

Associational questions (about categorical differences or correlations).  Example: How does the math achievement of the nation’s fourth grade Hispanic youth compare with that of the nation’s fourth grade African-American youth?  Are District of Columbia computer industry professional employees more or less satisfied with their careers than are DC media industry professional employees?

Causal questions (about what causes observed differences or changes).  Example: What accounts for the wide variation in math achievement among fourth grade Hispanic youth?  Will two hours a week of Math Explosion exercises over a semester raise the math achievement of low-achieving fourth grade Hispanic youth?

Benefit-cost questions (about whether the benefits of a given intervention are worth more than the costs).  Example: Do the benefits of Math Buster exercises exceed the costs of the computer laboratory, software, and laboratory supervision?  Is the benefit/cost ratio of Math Explosion exercises greater than the benefit/cost ratio of after school tutoring by the teachers?

The first listed types of questions are generally easier to answer than the latter ones.  For descriptive questions, there is only the need to measure the characteristics of interest in the population of interest.  For associational questions, categorical comparisons or correlations then are computed for the measured characteristics.  To answer causal questions, there is a need to establish (a) association, (b) temporal ordering (the alleged causal variables must occur before the outcome variables of interest), and (c) isolation (ascertaining that other variables may not have caused the outcomes).  For benefit-cost questions, there is also a need to compare the value of the benefits to the costs of the intervention program. 

Assessment QuestionsKey Assessment Questions

What are the questions investigated or hypotheses tests?  (Make sure to distinguish among the four types of questions indicated above.) 

* # LESSON A-4

Assessing the Contexts
of the Situation Studied

All phenomena occur in contexts that may affect the phenomena.  Experimental research strives to control the context and thus the effects it might have on the results, but that can rarely be done entirely.  For instance, if male and female freshmen are tested in laboratory settings for responses to violent media, their reactions may differ from those in unobserved settings, and the differences for males may be greater or less than for females. 

Qualitative research usually considers the contexts to be part of the phenomena of interest. 

Since contexts may affect the results of a study and they usually vary within a set of studies on a given topic, it is important to take into account the contexts of studies when engaging in the integration phase of the review.  There are hundreds of contexts that might affect the results of a study including the time in history, geography, social-economic conditions, cultural values, current events, and the array of other interventions to which students or employees may be subject.  Which ones are likely to most affect the results is impossible to know with certainty, but prior research, theory, and intuition can help in identifying them.  Note that the characteristics of the people, schools, businesses, or other units that are sampled are not properly considered contexts, but rather population characteristics, and will be discussed below in Lesson A-6.  While varying contexts in a set of studies on a topic can cause varying results, that will not always be the case.  Sometimes the results are fairly consistent across studies undertaken in different contexts, and when so, this strongly suggests the observed findings are robust-not much affected by contexts. 

Assessment Questions Key Assessment Questions

What are the contexts of the study that may have affected the results? 

* # LESSON A-5

Assessing the Interventions (When
Applicable) That Were Studied

Some research examines the effects of an intervention.  The intervention may be changes in the physical or social environment, implementation of a new policy, the introduction of an innovative program, or an experimental treatment.  The intervention may be naturally occurring, humanly induced (but not at the behest or within the control of the researcher), or it may be induced specifically for the research.  While medical research and laboratory psychology experiments can often describe the intervention in one paragraph, that is rarely the case in most educational, human development, and human resource development research. 

To adequately understand an intervention, we generally need to know the following: 

What was done or happened? 
With what intensity?
For what duration?
How? 
Where?
When? 
To whom?
With what variations for all the above? 

One serious problem in educational and worksite demonstrations and experiments is that complex interventions are often not implemented as intended, and that is not reported by the researchers.  The results-whether favorable or unfavorable-are mistakenly attributed to the planned intervention rather than the actual one.

The best evidence of the actual intervention is detailed data collection on the above listed characteristics at several points in time over the period the intervention is in place.  This is commonly done in qualitative evaluations but less commonly done in quantitative research and evaluation.  The second best evidence of the actual intervention is an indication that checks were made to ascertain that the intervention did occur as expected.  The third best evidence, applicable to demonstrations and experiments, is an indication that care was taken to train the personnel responsible for implementing the innovation, providing them with the tools needed for implementation, monitoring of implementation, and offering corrective feedback when implementation began to go astray. 

Sometimes the intended intervention is deliberately altered during the study to correct shortcomings and take advantage of opportunities that had not initially been recognized.  That is fine, provided the actual intervention, as it evolved over time, is well documented.  More often, educational and human resource interventions fall short of the plans because of inadequate planning, training, and monitoring; because of insufficient resources; or because of a lack of cooperation and sometimes sabotage by those disapproving of the intervention.  It is important to know when that is the case, but if the researchers are responsible for the intervention, there is a tendency for them not to report their implementation shortcomings. 

Assessment Questions Key Assessment Questions

What intervention was actually implemented, with what intensity and duration, and how did it perhaps vary across subjects, time, and other important dimensions?

* # LESSON A-6

Assessing the Methodology
of the Study

There are four main aspects of the research methodology: design, sampling, data collection, the data analysis.  If inappropriate methodology is used, or if appropriate methodology is used poorly, the results of a study could be misleading. 
Design

Research design specifies what group(s) for which data will be collected, to which group(s) and when the intervention will occur, and when the data will collected from each group.  The strength of a design, and the possible biases inherent in a design, depend on the type of questions being addressed in the research.

Descriptive and associational questions need designs that only specify when the data will be collected from one group of interest.

Causal questions are usually answered by designs where:

the intervention is applied to one group of two comparable groups, and measures of expected outcomes are made for both groups at the end of the intervention; 
repeated measures of the outcomes of interest are made several times before and after the intervention is applied to one group; or, 

for a group in which some units have received the intervention (perhaps varying levels of it) and some units have not, data is collected at one point in time on suspected causal variables, on the outcome variables of interest, and on various other variables that might have affected the outcomes of interest, and then the data are analyzed to determine whether the suspected causal elements actually had an effect on the outcomes.

Benefit-cost analyses require the designs for causal questions plus collection of data that permits calculations of the value of the benefits as well as the costs.

It should be noted that “experimental designs” are sometimes alleged to be the “gold standard” in the social sciences.  This is nonsense.  Experimental designs are not needed to answer descriptive and associational questions, and they can do only part of what is needed in cost-benefit analyses.  Their potential strength is only in answering causal questions, and their power for that is easily compromised when researching complex educational or workplace innovations.  The gold standard in medicine is the “double-blind, placebo-controlled experiment” that is commonly used to test new medications (but not to test new surgical procedures-for reasons that should soon be apparent).  Subjects are assigned at random to treatment or control, half are given the new pill and half are given a similar pill that is inert.  The people providing the pills and instructions to the patients don’t know whether they are handing over the medicine or placebo (they are “blind to that”).  Likewise, the people who later measure the potential impacts on the medical condition and the possible side effects, don’t know which patients actually received the medication and which did not. 

These conditions rarely can prevail when testing complex educational or workplace innovations.  It is rare that a convincing placebo can be concocted and administered.  It is often difficult to prevent some spill-overs of the treatment--whereby those receiving the treatment share it with some friends who are in the control group.  It is also difficult to prevent those not receiving the treatment from seeking alternative treatments on their own.  It is rare that those administering the innovation can do so without knowing they are using the “treatment” rather than the “placebo.”  It is also rare that those measuring the effects are “blind” about who did and did not receive the treatment, although this is sometimes feasible to arrange.  This is not to say that experimental designs are a waste of time in answering causal questions in education and worksite research.  Sometimes they are the best option, but rarely are they “golden.”

In qualitative research, often the specific questions of interest emerge in the course of the study and thus the design for answering them must also emerge.  While the designs described above tend to be explicitly discussed in quantitative research, they can be applicable to qualitative research.  For instance, if the main question is what are recent Central American immigrant youth’s perceptions of DC school life, a phenomenologist could intensively study the perceptions several such youth already in one or more DC schools.  If the main question is whether Math Explosion software can boost these youths’ math skills, an ethnographer would have a stronger basis for answering the question if he or she intensively studied these youths’ application of math in school and outside, for awhile before the youth start using the software, during the use, and then afterwards.

Sampling

Sometimes a study involves the entire population of interest, but more often it involves only a small portion of the students, employees, families, schools, communities, or other “units of analysis.” Sampling serves three purposes:

It reduces the costs and time required to do the research;

It often improves the quality of information by allowing more intensive data collection than would otherwise be possible; and,
it reduces the burden on respondents. 

There are four main steps to sampling that are important to the interpretation of the results.  There may be weaknesses in one or more of the steps.  The terminology and procedures of sampling differ some between quantitative and qualitative research, and the quantitative framework is used immediately below.  The phases are:

Specification of a “population” (or “universe”) to which you wish to generalize.  One cannot properly make inferences beyond the population that was sampled.

Identification of a sampling frame of the population which lists all the persons, families, etc. in the desired population.  Often no perfect frame exists, and available or compiled lists include some people not in the population, and perhaps some people are listed more than once.

Drawing the sample.  Quantitative research using inferential statistics requires random sampling; qualitative research usually uses non-random procedures.

Securing the needed data from the sample.  Usually not all people included in a sample can be contacted and are willing to participate in the data collection.  Some that do participate will fail to provide some of the needed data, either because they do not know the information or they do not want to divulge it.  Response rates in surveys and long-term follow-ups of experiments are often very low (15-30 percent), and often it is difficult to ascertain whether they are representative of the other 70-85 percent of the people. 

Most quantitative research in education, human development, and human resource development falls short of what is needed for a solid sample.  Most do not sample randomly from a frame that closely coincides with the population of interest, but rather "conveniently" select several schools, homes, or worksites that are located near the researcher and agree to participate.  For long interventions and long-term follow-ups, some data is often missing for a substantial percentage of the sample.  To prevent these shortcomings usually would greatly increase the cost of the study.  Although one can never know with certainty, sometimes post-hoc analyses comparing characteristics of the sampled units with the population, and characteristics of the respondents with the initial sample, can suggest that one or both are representative.  Without such evidence,caution should caution be used in generalizing the results beyond the cases actually studied. 

Qualitative research, and some quantitative research, use non-random samples.  Non-random samples include quota samples in which the researcher selects participants in proportion to one or more characteristics of the population.  Typical case samples are drawn to represent the median characteristics of the population.  Critical cases are drawn to represent certain subgroups of the population that are of particular interest. All these have merit.

The sampling done in qualitative research, however, is often problematic for at least two reasons.  First, the researcher may consciously or subconsciously draw cases partly for reasons other than the stated one.  For instance, an ethnographer investigating suspected adverse effects of state education reforms on minority youth, may select the few classrooms for intense observation not only to be “typical” but also partly because they are known not to be handling the reforms well.  Second, qualitative researchers often don’t explain how they selected the people to observe or to interview, and they rarely tell you what portion of those initially selected refused to cooperate.  Consequently, it often is difficult to judge the adequacy of sampling in qualitative research. 

Data Collection

Quantitative researchers develop most of their questions and hypotheses very specifically before the study, and then find or develop instruments for collecting the data.  That gives them opportunity to refine each item, but no opportunity to address new questions that may arise from the early data collection.  Qualitative researchers usually start with a qualitative research methodology (such as historiography, ethnography, phenomenology) and often an interpretive paradigm, and then collect data intensively by observation and unstructured interviews.  That allows them to use early findings to generate new questions that they examine in the later stages of data collection, but they often have to focus their observations and develop their interview questions on the fly without any opportunity to refine them.

The means of data collection in social science are diverse.  For instance, one can observe and code or note, administer tests of skills, administer various personality and attitude inventories, interview people in person or by phone, mail out questionnaires, content-analyze transcripts of dialogue, and review official documents.

There are two key elements of data collection in quantitative research: the instruments and the data collection procedures.  The term “instruments” in the social sciences usually refers to written forms on which the researchers or the people being studied record information.  Mechanical and electrical measure are also occasionally used. 

Two concepts are central to quantitative measurement: reliability and validity.  Reliability means the instrument consistently gives the same value when measuring any given level of a phenomena.  Validity means that the value yielded by the instrument is accurate.  Reliability is necessary but not sufficient for valid measurement.  For instance, careful use of a ruler will allow measurements accurate within about 1/16 of an inch, but the measurements will not be accurate if the user unknowingly has a ruler that has shrunk one inch.  Some measures in quantitative social science have credible evidence of their reliability and validity, but most do not and thus must be judged on whatever is apparent from reviewing them.  Do the instruments seem to cover all the important issues?  Is there balance or do most of the items address only strengths or weaknesses?  Is a wide range of responses, ratings, scores, etc. possible?  Are the instruments easy to use correctly?  Were new instruments developed specifically for the study pilot tested?  Who collected the data, with what advance training, and what introductions and instructions provided to the participants, and with what monitoring of the data collection?

Qualitative research relies much less on instruments, making the procedures all-important.  The data collection is usually done by doctoral students or scholars, rather than delegated to people with lesser research experience-which is often done in quantitative research.  Qualitative research reports usually provide a very general idea of how the data was collected but provide few specifics.  These reports rarely indicate what questions were posed in the interviews-indeed the questions often vary from one interviewee to the other, making a report of the questions impractical.  The reports also rarely indicate what potentially important events were not observed because of various constraints.  Often the only practical way to assess qualitative research data collection is to check whether the investigator at least sought data to challenge or verify his or her early results.

Virtually all data collection methods have their shortcomings and potential biases. Experienced researchers, both quantitative and qualitative, know it is best to try to measure the most important variables with multiple items and/or multiple means, and then compare the results. 

Data Analysis

In quantitative research, well established statistical procedures are usually used.  The appropriateness of the selected procedures can be judged by two criteria. The first is whether the design and data meet the assumptions of the procedure.  Some of the more common assumptions are in respect to the level of measurement (nominal ordinal, interval and ratio), normality of distributions (for parametric statistics), and homogeneity of variance (for ANOVA). The second criteria is whether the selected statistical procedure is the most powerful of the available procedures whose assumptions were met. 

There is an important aspect of quantitative data analysis that is more difficult to judge-the care with which the data were handled before the analysis and the care with which the data analysis was actually conducted.  Manually recorded data almost always includes errors.  Some of the errors can be identified by reviewing the data forms, and for some of those identified errors, the correct value can be inferred.  Data entry into the computer usually results in some errors, and those can be detected by a second independent keying and automatic check, or by visual comparison of the data forms and the computer record. Some additional data errors can be identified by computer edits for values that are out of the eligible range or inconsistent with each other. In addition to data errors, there can be errors in the commands given to the statistical software. The classic warning of professional data processors is “Garbage In, Garbage Out (GIGO).” 

The reader of a research report may detect some errors from implausible results or inconsistencies within or between the tabulated results. Otherwise the best assessment of the data handling is to look in the report for an indication that the data were manually edited, the data entry was verified, and the data file was subjected to further computer edits before the analyses began.

The data analysis of qualitative research is generally inductive, interactive, and iterative. It usually involves the identification of categories, themes, relations among both, and the cross verification of tentative answers to descriptive, associational, and causal questions. The analysis is often described or implied in the discussion of the findings. Competent and careful qualitative data analysis is usually indicated by the researcher exhibiting healthy skepticism, drawing on multiple lines of evidence, and testing his or her early findings with subsequent evidence.

Key Assessment Questions

Is the design suitable for the types of questions to be answered?

Is the sample likely to be representative of the population or sub-population of interest and was data secured from a large portion of the initial sample?

Are the data collection instruments and procedures likely to have measured all the important characteristics with reasonable accuracy?

Are the quantitative analysis procedures appropriate, does the qualitative analysis cross-verify important findings, and does all the data analysis appear to have been done with care? 

Note: If the information in the research report does not satisfactorily answer most of these questions, you should worry about the quality of the methodology.  Nevertheless, that is not sufficient evidence to conclude that the research methodology is inadequate.

* # LESSON A-7

Assessing the Results of the Study 

There are five key characteristics of research results: direction, magnitude, variance (of averages and summaries), statistical significance (when hypothesis testing), and consistency within the study.  The importance and implications of the results depends on all five. 

While the importance of the direction and magnitude of results is obvious, these characteristic sometimes go unreported.  There are thousands of correlations reported in the research literature without an indication of whether they were positive or negative correlations.  There are thousands of experimental differences reported as statistically significant without an indication of the magnitude of the difference.  Qualitative research is usually reported in more detail so direction rarely goes unreported.  Qualitative analysis, however, often provides ambiguous indications of magnitude.  If it says, “the entry-level employees were initially delighted by the firm’s offer to provide tuition assistance at the local community college” does that mean all, most or many of the entry level employees?  And if the qualifier “most” is added to the statement, does that mean at least 90 percent, 80 percent, or 70 percent? 

Averages or summaries simplify complexity and that is often helpful, but they can also hide important information.  It is very easy to accompany mean values with the variance, and that adds considerable information.  Some qualitative researchers are careful to report observed variations, but others, in their effort to indicate themes and patterns, don’t provide a sense of the variation. 

While statistical significance is usually reported in quantitative research, it is widely misinterpreted by researchers and by readers.  When the samples are small (less than 100), a failure to find statistical significance may mean there is no difference in the population or it may mean that there is a modest difference that was not inferred because of the inadequate power of the test.  The power of all statistical tests is positively affected by sample size and inversely affected by the variance in the samples.  Conversely, a finding of statistical significance from large samples (greater than 1,000) can result from trivial differences.  Furthermore, it should also be noted that when 100 hypotheses are tested at the .05 level of significance, there is a good chance of finding about 5 statistically significant results by chance when there are no real differences in the population.

Most quantitative research involves multiple results, and the pattern of the results is far more important than any one by itself.  For that reason, it is important that the report indicate all the results, but that may not be the case.  Researchers will usually report all their statistically significant results, but they sometimes fail to report some or all of the results that were not significant.  The following example will illustrate how this can be grossly misleading. In a study of the effects of Math Explosion software on Hispanic youths, it might be reported that the intervention had statistically significant results for mathematical computation skills.  That seems interesting and encouraging until you learn that four other measures of math achievement did not show statistically significant results!  The full set of measures might have included a standardized achievement test with scores for computation, concepts, and problem-solving; the students’ math course grades; and their quantitative score on the SAT college admissions test. 

Qualitative researchers don’t have any device comparable to statistical significance.  That makes the pattern of their results even more important than in quantitative research.  Good qualitative research explicitly cross- verifies important results (“triangulate”) and indicates when some but not all of the evidence points in a given direction. 

How can you ascertain whether a research report indicates all the results?  Sometimes it will say that is has or has not.  Sometimes more measures are discussed in the data collection section than are reported in the results section of the report.  In addition, the following three conditions that should raise concerns about selective reporting of the results: 

Do all the reported hypothesis tests have statistically significant results?  Rarely are all the results significant except when five or more significance tests are conducted.

Are all the reported results consistent?  For instance, do all the results favor a demonstrated innovation or do all show a given social arrangement to be dysfunctional?  Since social phenomena are rarely perfectly consistent, high consistency in the results suggests selective reporting by the researcher.

Is there an indication that the researcher was surprised by some of the results? This is usually found toward the end of the report.  It is almost impossible to do honest research without encountering some surprises and being intrigued by them.  If the researcher found no surprises, it is possible that the whole study was skewed to support his or her pre-held opinions or that selective reporting buried unwanted results.

Just as the pattern of results within each study is important for interpreting a given study, during the integration stage of a literature review, the larger pattern of results across studies becomes critical.  That will be discussed in the following section of this Web site. 

Assessment QuestionsKey Assessment Questions

What is the direction, magnitude, variance, and statistical significance (when applicable) of each result? 

What is the pattern of results for each broad question that was addressed?

* # LESSON A-8

Assessing the Conclusions of the Study 

Conclusions pull together the various results of the study, consider what they mean, and suggest their importance.  There are several types of conclusions.  The following is one typology: 
Findings summarize two or more results.

Interpretations indicate what the results and findings mean within the study and within the context of prior research on the topic.

Generalization extrapolates the results or findings beyond the studied units.

Implications suggest how the findings may be important for policy, practice, theory, and subsequent research.

Recommendations urge specific actions in respect to policy, practice, theory, or subsequent research.

Conclusions have considerable appeal-indeed they are the objective of any research study.  They are also usually easily read.  For those reasons, apprentice scholars may skip most of the research report and jump to the conclusions and recommendations.  That is a mistake. 

Conclusions and recommendations are not an automatic extension of the results.  They require careful inference in light of the delimitations of the study (the bounds on the questions of the study, the contexts studied, the population and sampling frame, and the interventions that occurred or were administered) and in light of the limitations of the study (the methodological shortcomings).  They also require broad knowledge of the topic being addressed, they require combining facts and values, and they involve some speculation.

Even eminent researchers occasionally blow the conclusions and recommendations of a study.  It is not uncommon to find a conclusion or two with no real support from the results, and occasionally a conclusion will be contradicted by the results.  The most common reasons for that are fatigue and ambition.  The conclusions and recommendations cannot be generated until the rest of the research is completed, and by then the research has usually taken more time and money than expected.  So the conclusions and recommendations are often hastily assembled.  In addition, many researchers have hopes of making important contributions, and, at the end, they sometimes succumb to concluding and recommending more than is well justified by their study.

Whether there is good justification for the conclusions can usually be determined by reading the research report.  Indeed, readers may infer additional conclusions that are well supported by the study but not stated in the report.  It should be noted, however, that some omissions in the report may make some conclusions appear unjustified even though the researcher actually has good justification for them.

Common errors when generating the conclusions and recommendations are the following: 

Stating conclusions that the researcher thinks are correct and important, but for which the study provides no support.

Generalizing well beyond the questions, contexts, population, and interventions that were actually studied.

Not adding cautions when there are important limitations in methods and/or their execution in the study.

Falsely interpreting statistical significance and the lack thereof (This is explained in Lesson A-7).

Selectively focusing on some results while ignoring others and the pattern of results (See Lesson A-7).

Moving from inference to values and speculation, with using wording that clearly indicates that is involved.
Assessment QuestionsKey Assessment Questions

Which conclusions appear to be well supported by the pattern of results, the delimitations, and the limitations of the study; and which conclusions do not appear to be well supported? 

Which interpretations, implications, and recommendations are well supported according to Appraisal Question 12 and also are well supported by what is generally known about the problem and by prior research findings?

