<html>

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../styles.css">
  <link rel="icon" href="../assets/gw-classic.gif">
  <title>Doing Successful Literature Reviews</title>
</head>

<body>
  <table width="600">
    <tbody>
      <tr class="cent">
        <td colspan="2">
          <img src="../assets/litbanner.gif" usemap="#map" height="51" width="580">
        </td>
      </tr>

      <tr>
        <td width="420">
          <b class="good-blue bigger-font">
            Lesson A-6
          </b>

          <p class="cent good-blue big-font"><b>
              Assessing the Methodology
            </b>
            <br><b>
              of the Study
            </b>
          </p>

        </td>

        <td width="200"><img src="../assets/truth.gif" height="125" width="130"></td>
      </tr>

      <tr>
        <td colspan="2" height="896">There are four main aspects of the research
          methodology: design, sampling, data collection, the data analysis.&nbsp;
          If inappropriate methodology is used, or if appropriate methodology is
          used poorly, the results of a study could be misleading.&nbsp;
          <p><b>Design</b>
          </p>
          <p>Research design specifies what group(s) for which data will be collected,
            to which group(s) and when the intervention will occur, and when the data
            will collected from each group.&nbsp; The strength of a design, and the
            possible biases inherent in a design, depend on the type of questions being
            addressed in the research.
          </p>
          <ul>
            <li>
              Descriptive and associational questions need designs that only specify
              when the data will be collected from one group of interest.</li>

            <li>
              Causal questions are usually answered by designs where:</li>

            <ul>
              <li>
                the intervention is applied to one group of two comparable groups, and
                measures of expected outcomes are made for both groups at the end of the
                intervention;&nbsp;</li>

              <li>
                repeated measures of the outcomes of interest are made several times before
                and after the intervention is applied to one group; or,&nbsp;</li>

              <li>
                for a group in which some units have received the intervention (perhaps
                varying levels of it) and some units have not, data is collected at one
                point in time on suspected causal variables, on the outcome variables of
                interest, and on various other variables that might have affected the outcomes
                of interest, and then the data are analyzed to determine whether the suspected
                causal elements actually had an effect on the outcomes.</li>
            </ul>

            <li>
              Benefit-cost analyses require the designs for causal questions plus collection
              of data that permits calculations of the value of the benefits as well
              as the costs.</li>
          </ul>
          It should be noted that “experimental designs” are sometimes alleged to
          be the “gold standard” in the social sciences.&nbsp; This is nonsense.&nbsp;
          Experimental designs are not needed to answer descriptive and associational
          questions, and they can do only part of what is needed in cost-benefit
          analyses.&nbsp; Their potential strength is only in answering causal questions,
          and their power for that is easily compromised when researching complex
          educational or workplace innovations.&nbsp; The gold standard in medicine
          is the “double-blind, placebo-controlled experiment” that is commonly used
          to test new medications (but not to test new surgical procedures-for reasons
          that should soon be apparent).&nbsp; Subjects are assigned at random to
          treatment or control, half are given the new pill and half are given a
          similar pill that is inert.&nbsp; The people providing the pills and instructions
          to the patients don’t know whether they are handing over the medicine or
          placebo (they are “blind to that”).&nbsp; Likewise, the people who later
          measure the potential impacts on the medical condition and the possible
          side effects, don’t know which patients actually received the medication
          and which did not.&nbsp;
          <p>These conditions rarely can prevail when testing complex educational
            or workplace innovations.&nbsp; It is rare that a convincing placebo can
            be concocted and administered.&nbsp; It is often difficult to prevent some
            spill-overs of the treatment--whereby those receiving the treatment share
            it with some friends who are in the control group.&nbsp; It is also difficult
            to prevent those not receiving the treatment from seeking alternative treatments
            on their own.&nbsp; It is rare that those administering the innovation
            can do so without knowing they are using the “treatment” rather than the
            “placebo.”&nbsp; It is also rare that those measuring the effects are “blind”
            about who did and did not receive the treatment, although this is sometimes
            feasible to arrange.&nbsp; This is not to say that experimental designs
            are a waste of time in answering causal questions in education and worksite
            research.&nbsp; Sometimes they are the best option, but rarely are they
            “golden.”
          </p>
          <p>In qualitative research, often the specific questions of interest emerge
            in the course of the study and thus the design for answering them must
            also emerge.&nbsp; While the designs described above tend to be explicitly
            discussed in quantitative research, they can be applicable to qualitative
            research.&nbsp; For instance, if the main question is what are recent Central
            American immigrant youth’s perceptions of DC school life, a phenomenologist
            could intensively study the perceptions several such youth already in one
            or more DC schools.&nbsp; If the main question is whether Math Explosion
            software can boost these youths’ math skills, an ethnographer would have
            a stronger basis for answering the question if he or she intensively studied
            these youths’ application of math in school and outside, for awhile before
            the youth start using the software, during the use, and then afterwards.
          </p>
          <p><b>Sampling</b>
          </p>
          <p>Sometimes a study involves the entire population of interest, but more
            often it involves only a small portion of the students, employees, families,
            schools, communities, or other “units of analysis.” Sampling serves three
            purposes:
          </p>
          <ul>
            <li>
              It reduces the costs and time required to do the research;</li>

            <li>
              It often improves the quality of information by allowing more intensive
              data collection than would otherwise be possible; and,</li>

            <li>
              it reduces the burden on respondents.&nbsp;</li>
          </ul>
          There are four main steps to sampling that are important to the interpretation
          of the results.&nbsp; There may be weaknesses in one or more of the steps.&nbsp;
          The terminology and procedures of sampling differ some between quantitative
          and qualitative research, and the quantitative framework is used immediately
          below.&nbsp; The phases are:
          <ol>
            <li>
              <b>Specification of a “population” </b>(or “universe”) to which you wish
              to generalize.&nbsp; One cannot properly make inferences beyond the population
              that was sampled.<br clear="BOTH">
              <br>
            </li>

            <li>
              <b>Identification of a sampling frame</b> of the population which lists
              all the persons, families, etc. in the desired population.&nbsp; Often
              no perfect frame exists, and available or compiled lists include some people
              not in the population, and perhaps some people are listed more than once.<br clear="BOTH">
              <br>
            </li>

            <li>
              <b>Drawing the sample</b>.&nbsp; Quantitative research using inferential
              statistics requires random sampling; qualitative research usually uses
              non-random procedures.<br clear="BOTH">
              <br>
            </li>

            <li>
              <b>Securing the needed data</b> from the sample.&nbsp; Usually not all
              people included in a sample can be contacted and are willing to participate
              in the data collection.&nbsp; Some that do participate will fail to provide
              some of the needed data, either because they do not know the information
              or they do not want to divulge it.&nbsp; Response rates in surveys and
              long-term follow-ups of experiments are often very low (15-30 percent),
              and often it is difficult to ascertain whether they are representative
              of the other 70-85 percent of the people.&nbsp;
            </li>
          </ol>
          Most quantitative research in education, human development, and human resource
          development falls short of what is needed for a solid sample.&nbsp; Most
          do not sample randomly from a frame that closely coincides with the population
          of interest, but rather "conveniently" select several schools, homes, or
          worksites that are located near the researcher and agree to participate.&nbsp;
          For long interventions and long-term follow-ups, some data is often missing
          for a substantial percentage of the sample.&nbsp; To prevent these shortcomings
          usually would greatly increase the cost of the study.&nbsp; Although one
          can never know with certainty, sometimes post-hoc analyses comparing characteristics
          of the sampled units with the population, and characteristics of the respondents
          with the initial sample, can suggest that one or both are representative.&nbsp;
          Without such evidence,caution should caution be used in generalizing the
          results beyond the cases actually studied.&nbsp;
          <p>Qualitative research, and some quantitative research, use non-random
            samples.&nbsp; Non-random samples include quota samples in which the researcher
            selects participants in proportion to one or more characteristics of the
            population.&nbsp; Typical case samples are drawn to represent the median
            characteristics of the population.&nbsp; Critical cases are drawn to represent
            certain subgroups of the population that are of particular interest. All
            these have merit.
          </p>
          <p>The sampling done in qualitative research, however, is often problematic
            for at least two reasons.&nbsp; First, the researcher may consciously or
            subconsciously draw cases partly for reasons other than the stated one.&nbsp;
            For instance, an ethnographer investigating suspected adverse effects of
            state education reforms on minority youth, may select the few classrooms
            for intense observation not only to be “typical” but also partly because
            they are known not to be handling the reforms well.&nbsp; Second, qualitative
            researchers often don’t explain how they selected the people to observe
            or to interview, and they rarely tell you what portion of those initially
            selected refused to cooperate.&nbsp; Consequently, it often is difficult
            to judge the adequacy of sampling in qualitative research.&nbsp;
          </p>
          <p><b>Data Collection</b>
          </p>
          <p>Quantitative researchers develop most of their questions and hypotheses
            very specifically before the study, and then find or develop instruments
            for collecting the data.&nbsp; That gives them opportunity to refine each
            item, but no opportunity to address new questions that may arise from the
            early data collection.&nbsp; Qualitative researchers usually start with
            a qualitative research methodology (such as historiography, ethnography,
            phenomenology) and often an interpretive paradigm, and then collect data
            intensively by observation and unstructured interviews.&nbsp; That allows
            them to use early findings to generate new questions that they examine
            in the later stages of data collection, but they often have to focus their
            observations and develop their interview questions on the fly without any
            opportunity to refine them.
          </p>
          <p>The means of data collection in social science are diverse.&nbsp; For
            instance, one can observe and code or note, administer tests of skills,
            administer various personality and attitude inventories, interview people
            in person or by phone, mail out questionnaires, content-analyze transcripts
            of dialogue, and review official documents.
          </p>
          <p>There are two key elements of data collection in quantitative research:
            the instruments and the data collection procedures.&nbsp; The term “instruments”
            in the social sciences usually refers to written forms on which the researchers
            or the people being studied record information.&nbsp; Mechanical and electrical
            measure are also occasionally used.&nbsp;
          </p>
          <p>Two concepts are central to quantitative measurement: reliability and
            validity.&nbsp; Reliability means the instrument consistently gives the
            same value when measuring any given level of a phenomena.&nbsp; Validity
            means that the value yielded by the instrument is accurate.&nbsp; Reliability
            is necessary but not sufficient for valid measurement.&nbsp; For instance,
            careful use of a ruler will allow measurements accurate within about 1/16
            of an inch, but the measurements will not be accurate if the user unknowingly
            has a ruler that has shrunk one inch.&nbsp; Some measures in quantitative
            social science have credible evidence of their reliability and validity,
            but most do not and thus must be judged on whatever is apparent from reviewing
            them.&nbsp; Do the instruments seem to cover all the important issues?&nbsp;
            Is there balance or do most of the items address only strengths or weaknesses?&nbsp;
            Is a wide range of responses, ratings, scores, etc. possible?&nbsp; Are
            the instruments easy to use correctly?&nbsp; Were new instruments developed
            specifically for the study pilot tested?&nbsp; Who collected the data,
            with what advance training, and what introductions and instructions provided
            to the participants, and with what monitoring of the data collection?
          </p>
          <p>Qualitative research relies much less on instruments, making the procedures
            all-important.&nbsp; The data collection is usually done by doctoral students
            or scholars, rather than delegated to people with lesser research experience-which
            is often done in quantitative research.&nbsp; Qualitative research reports
            usually provide a very general idea of how the data was collected but provide
            few specifics.&nbsp; These reports rarely indicate what questions were
            posed in the interviews-indeed the questions often vary from one interviewee
            to the other, making a report of the questions impractical.&nbsp; The reports
            also rarely indicate what potentially important events were not observed
            because of various constraints.&nbsp; Often the only practical way to assess
            qualitative research data collection is to check whether the investigator
            at least sought data to challenge or verify his or her early results.
          </p>
          <p>Virtually all data collection methods have their shortcomings and potential
            biases. Experienced researchers, both quantitative and qualitative, know
            it is best to try to measure the most important variables with multiple
            items and/or multiple means, and then compare the results.&nbsp;
          </p>
          <p><b>Data Analysis</b>
          </p>
          <p>In quantitative research, well established statistical procedures are
            usually used.&nbsp; The appropriateness of the selected procedures can
            be judged by two criteria. The first is whether the design and data meet
            the assumptions of the procedure.&nbsp; Some of the more common assumptions
            are in respect to the level of measurement (nominal ordinal, interval and
            ratio), normality of distributions (for parametric statistics), and homogeneity
            of variance (for ANOVA). The second criteria is whether the selected statistical
            procedure is the most powerful of the available procedures whose assumptions
            were met.&nbsp;
          </p>
          <p>There is an important aspect of quantitative data analysis that is more
            difficult to judge-the care with which the data were handled before the
            analysis and the care with which the data analysis was actually conducted.&nbsp;
            Manually recorded data almost always includes errors.&nbsp; Some of the
            errors can be identified by reviewing the data forms, and for some of those
            identified errors, the correct value can be inferred.&nbsp; Data entry
            into the computer usually results in some errors, and those can be detected
            by a second independent keying and automatic check, or by visual comparison
            of the data forms and the computer record. Some additional data errors
            can be identified by computer edits for values that are out of the eligible
            range or inconsistent with each other. In addition to data errors, there
            can be errors in the commands given to the statistical software. The classic
            warning of professional data processors is “Garbage In, Garbage Out (GIGO).”&nbsp;
          </p>
          <p>The reader of a research report may detect some errors from implausible
            results or inconsistencies within or between the tabulated results. Otherwise
            the best assessment of the data handling is to look in the report for an
            indication that the data were manually edited, the data entry was verified,
            and the data file was subjected to further computer edits before the analyses
            began.
          </p>
          <p>The data analysis of qualitative research is generally inductive, interactive,
            and iterative. It usually involves the identification of categories, themes,
            relations among both, and the cross verification of tentative answers to
            descriptive, associational, and causal questions. The analysis is often
            described or implied in the discussion of the findings. Competent and careful
            qualitative data analysis is usually indicated by the researcher exhibiting
            healthy skepticism, drawing on multiple lines of evidence, and testing
            his or her early findings with subsequent evidence.
          </p>
          <dl>
            <dt>
              <img src="../assets/scale.gif" height="26" width="30"><b>Key Assessment
                Questions</b>
            </dt>

            <dd>
              6. Is the design suitable for the types of questions to be answered?</dd>

            <dl>7. Is the sample likely to be representative of the population or sub-population
              of interest and was data secured from a large portion of the initial sample?
              <br>8. Are the data collection instruments and procedures likely to have
              measured all the important characteristics with reasonable accuracy?
              <br>9. Are the quantitative analysis procedures appropriate, does the qualitative
              analysis cross-verify important findings, and does all the data analysis
              appear to have been done with care?&nbsp;
            </dl>
          </dl>
          Note: If the information in the research report does not satisfactorily
          answer most of these questions, you should worry about the quality of the
          methodology.&nbsp; Nevertheless, that is not sufficient evidence to conclude
          that the research methodology is inadequate.
          <div class="cent">
            <p><b><a href="../index.html">Return
                  Home</a></b>
              or <b><a href="../lessons/a7.html">Advance to
                  Lesson A-7</a></b></p>
          </div>

          <hr>
          <p>Last Update: June 29, 2000&nbsp;
            <map name="map">
              <area shape="rect" coords="459,32,576,48"
                href="https://web.archive.org/web/20190623070040/http://www.gwu.edu/"
                alt="Link to the George Washington University">
              <area shape="rect" coords="459,17,578,32" href="mailto:frw@gwu.edu" alt="Send feedback">
              <area shape="rect" coords="459,0,578,17"
                href="https://web.archive.org/web/20190623070040/http://www.gwu.edu/~edpol/"
                alt="Link to Education Policy Page">
            </map>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>